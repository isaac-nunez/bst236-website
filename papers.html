<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Papers</title>
    <link rel="stylesheet" href="styles/style.css">
</head>
<body>
    <div class="dog-container">
        <img src="images/dog.jpg" alt="My dog" class="dog-image">
        <p class="dog-caption">The guardian is observing</p>
    </div>
    <header>
        <h1>Papers</h1>
        <nav>
            <ul>
                <li><a href="main.html">Main (regular stuff)</a></li>
                <li><a href="games.html">Games (fun stuff)</a></li>
                <li><a href="papers.html">Papers (sciency stuff)</a></li>
            </ul>
        </nav>
    </header>
    <main>
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section id="papers-section">
            <h2>Latest Papers in Causal Inference</h2>
            <ul id="papersList">
                <li>
                    <h3>RAD: Training an End-to-End Driving Policy via Large-Scale 3DGS-based
  Reinforcement Learning</h3>
                    <p><strong>Authors:</strong> Hao Gao, Shaoyu Chen, Bo Jiang, Bencheng Liao, Yiang Shi, Xiaoyang Guo, Yuechuan Pu, Haoran Yin, Xiangyu Li, Xinbang Zhang, Ying Zhang, Wenyu Liu, Qian Zhang, Xinggang Wang</p>
                    <p>  Existing end-to-end autonomous driving (AD) algorithms typically follow the
Imitation Learning (IL) paradigm, which faces challenges such as causal
confusion and the open-loop gap. In this work, we establish a 3DGS-based
closed-loop Reinforcement Learning (RL) training paradigm. By leveraging 3DGS
techniques, we construct a photorealistic digital replica of the real physical
world, enabling the AD policy to extensively explore the state space and learn
to handle out-of-distribution scenarios through large-scale trial and error. To
enhance safety, we design specialized rewards that guide the policy to
effectively respond to safety-critical events and understand real-world causal
relationships. For better alignment with human driving behavior, IL is
incorporated into RL training as a regularization term. We introduce a
closed-loop evaluation benchmark consisting of diverse, previously unseen 3DGS
environments. Compared to IL-based methods, RAD achieves stronger performance
in most closed-loop metrics, especially 3x lower collision rate. Abundant
closed-loop results are presented at https://hgao-cv.github.io/RAD.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.13144v1" target="_blank">Read PDF</a></p>
                    <p><strong>Submitted on:</strong> 2/18/2025</p>
                </li>
                <li>
                    <h3>RHINO: Learning Real-Time Humanoid-Human-Object Interaction from Human
  Demonstrations</h3>
                    <p><strong>Authors:</strong> Jingxiao Chen, Xinyao Li, Jiahang Cao, Zhengbang Zhu, Wentao Dong, Minghuan Liu, Ying Wen, Yong Yu, Liqing Zhang, Weinan Zhang</p>
                    <p>  Humanoid robots have shown success in locomotion and manipulation. Despite
these basic abilities, humanoids are still required to quickly understand human
instructions and react based on human interaction signals to become valuable
assistants in human daily life. Unfortunately, most existing works only focus
on multi-stage interactions, treating each task separately, and neglecting
real-time feedback. In this work, we aim to empower humanoid robots with
real-time reaction abilities to achieve various tasks, allowing human to
interrupt robots at any time, and making robots respond to humans immediately.
To support such abilities, we propose a general humanoid-human-object
interaction framework, named RHINO, i.e., Real-time Humanoid-human Interaction
and Object manipulation. RHINO provides a unified view of reactive motion,
instruction-based manipulation, and safety concerns, over multiple human signal
modalities, such as languages, images, and motions. RHINO is a hierarchical
learning framework, enabling humanoids to learn reaction skills from
human-human-object demonstrations and teleoperation data. In particular, it
decouples the interaction process into two levels: 1) a high-level planner
inferring human intentions from real-time human behaviors; and 2) a low-level
controller achieving reactive motion behaviors and object manipulation skills
based on the predicted intentions. We evaluate the proposed framework on a real
humanoid robot and demonstrate its effectiveness, flexibility, and safety in
various scenarios.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.13134v1" target="_blank">Read PDF</a></p>
                    <p><strong>Submitted on:</strong> 2/18/2025</p>
                </li>
                <li>
                    <h3>AV-Flow: Transforming Text to Audio-Visual Human-like Interactions</h3>
                    <p><strong>Authors:</strong> Aggelina Chatziagapi, Louis-Philippe Morency, Hongyu Gong, Michael Zollhoefer, Dimitris Samaras, Alexander Richard</p>
                    <p>  We introduce AV-Flow, an audio-visual generative model that animates
photo-realistic 4D talking avatars given only text input. In contrast to prior
work that assumes an existing speech signal, we synthesize speech and vision
jointly. We demonstrate human-like speech synthesis, synchronized lip motion,
lively facial expressions and head pose; all generated from just text
characters. The core premise of our approach lies in the architecture of our
two parallel diffusion transformers. Intermediate highway connections ensure
communication between the audio and visual modalities, and thus, synchronized
speech intonation and facial dynamics (e.g., eyebrow motion). Our model is
trained with flow matching, leading to expressive results and fast inference.
In case of dyadic conversations, AV-Flow produces an always-on avatar, that
actively listens and reacts to the audio-visual input of a user. Through
extensive experiments, we show that our method outperforms prior work,
synthesizing natural-looking 4D talking avatars. Project page:
https://aggelinacha.github.io/AV-Flow/
</p>
                    <p><a href="http://arxiv.org/pdf/2502.13133v1" target="_blank">Read PDF</a></p>
                    <p><strong>Submitted on:</strong> 2/18/2025</p>
                </li>
                <li>
                    <h3>Learning to Defer for Causal Discovery with Imperfect Experts</h3>
                    <p><strong>Authors:</strong> Oscar Clivio, Divyat Mahajan, Perouz Taslakian, Sara Magliacane, Ioannis Mitliagkas, Valentina Zantedeschi, Alexandre Drouin</p>
                    <p>  Integrating expert knowledge, e.g. from large language models, into causal
discovery algorithms can be challenging when the knowledge is not guaranteed to
be correct. Expert recommendations may contradict data-driven results, and
their reliability can vary significantly depending on the domain or specific
query. Existing methods based on soft constraints or inconsistencies in
predicted causal relationships fail to account for these variations in
expertise. To remedy this, we propose L2D-CD, a method for gauging the
correctness of expert recommendations and optimally combining them with
data-driven causal discovery results. By adapting learning-to-defer (L2D)
algorithms for pairwise causal discovery (CD), we learn a deferral function
that selects whether to rely on classical causal discovery methods using
numerical data or expert recommendations based on textual meta-data. We
evaluate L2D-CD on the canonical T\"ubingen pairs dataset and demonstrate its
superior performance compared to both the causal discovery method and the
expert used in isolation. Moreover, our approach identifies domains where the
expert's performance is strong or weak. Finally, we outline a strategy for
generalizing this approach to causal discovery on graphs with more than two
variables, paving the way for further research in this area.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.13132v1" target="_blank">Read PDF</a></p>
                    <p><strong>Submitted on:</strong> 2/18/2025</p>
                </li>
                <li>
                    <h3>SongGen: A Single Stage Auto-regressive Transformer for Text-to-Song
  Generation</h3>
                    <p><strong>Authors:</strong> Zihan Liu, Shuangrui Ding, Zhixiong Zhang, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Dahua Lin, Jiaqi Wang</p>
                    <p>  Text-to-song generation, the task of creating vocals and accompaniment from
textual inputs, poses significant challenges due to domain complexity and data
scarcity. Existing approaches often employ multi-stage generation procedures,
resulting in cumbersome training and inference pipelines. In this paper, we
propose SongGen, a fully open-source, single-stage auto-regressive transformer
designed for controllable song generation. The proposed model facilitates
fine-grained control over diverse musical attributes, including lyrics and
textual descriptions of instrumentation, genre, mood, and timbre, while also
offering an optional three-second reference clip for voice cloning. Within a
unified auto-regressive framework, SongGen supports two output modes: mixed
mode, which generates a mixture of vocals and accompaniment directly, and
dual-track mode, which synthesizes them separately for greater flexibility in
downstream applications. We explore diverse token pattern strategies for each
mode, leading to notable improvements and valuable insights. Furthermore, we
design an automated data preprocessing pipeline with effective quality control.
To foster community engagement and future research, we will release our model
weights, training code, annotated data, and preprocessing pipeline. The
generated samples are showcased on our project page at
https://liuzh-19.github.io/SongGen/ , and the code will be available at
https://github.com/LiuZH-19/SongGen .
</p>
                    <p><a href="http://arxiv.org/pdf/2502.13128v1" target="_blank">Read PDF</a></p>
                    <p><strong>Submitted on:</strong> 2/18/2025</p>
                </li>
                <li>
                    <h3>Facilitating Long Context Understanding via Supervised Chain-of-Thought
  Reasoning</h3>
                    <p><strong>Authors:</strong> Jingyang Lin, Andy Wong, Tian Xia, Shenghua He, Hui Wei, Mei Han, Jiebo Luo</p>
                    <p>  Recent advances in Large Language Models (LLMs) have enabled them to process
increasingly longer sequences, ranging from 2K to 2M tokens and even beyond.
However, simply extending the input sequence length does not necessarily lead
to effective long-context understanding. In this study, we integrate
Chain-of-Thought (CoT) reasoning into LLMs in a supervised manner to facilitate
effective long-context understanding. To achieve this, we introduce
LongFinanceQA, a synthetic dataset in the financial domain designed to improve
long-context reasoning. Unlike existing long-context synthetic data,
LongFinanceQA includes intermediate CoT reasoning before the final conclusion,
which encourages LLMs to perform explicit reasoning, improving accuracy and
interpretability in long-context understanding. To generate synthetic CoT
reasoning, we propose Property-driven Agentic Inference (PAI), an agentic
framework that simulates human-like reasoning steps, including property
extraction, retrieval, and summarization. We evaluate PAI's reasoning
capabilities by assessing GPT-4o-mini w/ PAI on the Loong benchmark,
outperforming standard GPT-4o-mini by 20.0%. Furthermore, we fine-tune
LLaMA-3.1-8B-Instruct on LongFinanceQA, achieving a 24.6% gain on Loong's
financial subset.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.13127v1" target="_blank">Read PDF</a></p>
                    <p><strong>Submitted on:</strong> 2/18/2025</p>
                </li>
                <li>
                    <h3>Benchmarking MedMNIST dataset on real quantum hardware</h3>
                    <p><strong>Authors:</strong> Gurinder Singh, Hongni Jin, Kenneth M. Merz Jr</p>
                    <p>  Quantum machine learning (QML) has emerged as a promising domain to leverage
the computational capabilities of quantum systems to solve complex
classification tasks. In this work, we present first comprehensive QML study by
benchmarking the MedMNIST-a diverse collection of medical imaging datasets on a
127-qubit real IBM quantum hardware, to evaluate the feasibility and
performance of quantum models (without any classical neural networks) in
practical applications. This study explore recent advancements in quantum
computing such as device-aware quantum circuits, error suppression and
mitigation for medical image classification. Our methodology comprised of three
stages: preprocessing, generation of noise-resilient and hardware-efficient
quantum circuits, optimizing/training of quantum circuits on classical
hardware, and inference on real IBM quantum hardware. Firstly, we process all
input images in the preprocessing stage to reduce the spatial dimension due to
the quantum hardware limitations. We generate hardware-efficient quantum
circuits using backend properties expressible to learn complex patterns for
medical image classification. After classical optimization of QML models, we
perform the inference on real quantum hardware. We also incorporates advanced
error suppression and mitigation techniques in our QML workflow including
dynamical decoupling (DD), gate twirling, and matrix-free measurement
mitigation (M3) to mitigate the effects of noise and improve classification
performance. The experimental results showcase the potential of quantum
computing for medical imaging and establishes a benchmark for future
advancements in QML applied to healthcare.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.13056v1" target="_blank">Read PDF</a></p>
                    <p><strong>Submitted on:</strong> 2/18/2025</p>
                </li>
                <li>
                    <h3>LAMD: Context-driven Android Malware Detection and Classification with
  LLMs</h3>
                    <p><strong>Authors:</strong> Xingzhi Qian, Xinran Zheng, Yiling He, Shuo Yang, Lorenzo Cavallaro</p>
                    <p>  The rapid growth of mobile applications has escalated Android malware
threats. Although there are numerous detection methods, they often struggle
with evolving attacks, dataset biases, and limited explainability. Large
Language Models (LLMs) offer a promising alternative with their zero-shot
inference and reasoning capabilities. However, applying LLMs to Android malware
detection presents two key challenges: (1)the extensive support code in Android
applications, often spanning thousands of classes, exceeds LLMs' context limits
and obscures malicious behavior within benign functionality; (2)the structural
complexity and interdependencies of Android applications surpass LLMs'
sequence-based reasoning, fragmenting code analysis and hindering malicious
intent inference. To address these challenges, we propose LAMD, a practical
context-driven framework to enable LLM-based Android malware detection. LAMD
integrates key context extraction to isolate security-critical code regions and
construct program structures, then applies tier-wise code reasoning to analyze
application behavior progressively, from low-level instructions to high-level
semantics, providing final prediction and explanation. A well-designed factual
consistency verification mechanism is equipped to mitigate LLM hallucinations
from the first tier. Evaluation in real-world settings demonstrates LAMD's
effectiveness over conventional detectors, establishing a feasible basis for
LLM-driven malware analysis in dynamic threat landscapes.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.13055v1" target="_blank">Read PDF</a></p>
                    <p><strong>Submitted on:</strong> 2/18/2025</p>
                </li>
                <li>
                    <h3>QZO: A Catalog of 5 Million Quasars from the Zwicky Transient Facility</h3>
                    <p><strong>Authors:</strong> S. J. Nakoneczny, M. J. Graham, D. Stern, G. Helou, S. G. Djorgovski, E. C. Bellm, T. X. Chen, R. Dekany, A. Drake, A. A. Mahabal, T. A. Prince, R. Riddle, B. Rusholme, N. Sravan</p>
                    <p>  Machine learning methods are well established in the classification of
quasars (QSOs). However, the advent of light curve observations adds a great
amount of complexity to the problem. Our goal is to use the Zwicky Transient
Facility (ZTF) to create a catalog of QSOs. We process the ZTF DR20 light
curves with a transformer artificial neural network and combine the Pan-STARRS
(PS), AllWISE, and Gaia surveys with extreme gradient boosting. Using ZTF
g-band data with at least 100 observational epochs per light curve, we obtain
97% F1 score for QSOs. We find that with 3 day median cadence, a survey time
span of at least 900 days is required to achieve 90% QSO F1 score. However, one
can obtain the same score with a survey time span of 1800 days and the median
cadence prolonged to 12 days. We find that ZTF classification is superior to
the PS static bands, and on par with WISE and Gaia measurements. Additionally,
we find that the light curves provide the most important features for QSO
classification in the ZTF dataset. We robustly classify objects fainter than
the $5\sigma$ SNR limit at $g=20.8$ by requiring $g < \mathrm{n_{obs}} / 80 +
20.375$. For this sample, we run inference with added WISE observations, and
find 4,849,574 objects classified as QSOs. For 33% of QZO objects, with
available WISE data, we publish redshifts with estimated error $\Delta z/(1 +
z) = 0.14$.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.13054v1" target="_blank">Read PDF</a></p>
                    <p><strong>Submitted on:</strong> 2/18/2025</p>
                </li>
                <li>
                    <h3>Development of systematic uncertainty-aware neural network trainings for
  binned-likelihood analyses at the LHC</h3>
                    <p><strong>Authors:</strong>  CMS Collaboration</p>
                    <p>  We propose a neural network training method capable of accounting for the
effects of systematic variations of the data model in the training process and
describe its extension towards neural network multiclass classification. The
procedure is evaluated on the realistic case of the measurement of Higgs boson
production via gluon fusion and vector boson fusion in the $\tau\tau$ decay
channel at the CMS experiment. The neural network output functions are used to
infer the signal strengths for inclusive production of Higgs bosons as well as
for their production via gluon fusion and vector boson fusion. We observe
improvements of 12 and 16% in the uncertainty in the signal strengths for gluon
and vector-boson fusion, respectively, compared with a conventional neural
network training based on cross-entropy.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.13047v1" target="_blank">Read PDF</a></p>
                    <p><strong>Submitted on:</strong> 2/18/2025</p>
                </li>
            </ul>
        </section>
    </main>
    <script src="scripts/papers.js"></script>
</body>
</html>