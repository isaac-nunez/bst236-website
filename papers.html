<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Papers</title>
    <link rel="stylesheet" href="styles/style.css">
</head>
<body>
    <div class="dog-container">
        <img src="images/dog.jpg" alt="My dog" class="dog-image">
        <p class="dog-caption">The guardian is observing</p>
    </div>
    <header>
        <h1>Papers</h1>
        <nav>
            <ul>
                <li><a href="main.html">Main (regular stuff)</a></li>
                <li><a href="games.html">Games (fun stuff)</a></li>
                <li><a href="papers.html">Papers (sciency stuff)</a></li>
            </ul>
        </nav>
    </header>
    <main>
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section id="papers-section">
            <h2>Latest Papers in Causal Inference</h2>
            <ul id="papersList">
                <li>
                    <h3>Scalable Equilibrium Sampling with Sequential Boltzmann Generators</h3>
                    <p><strong>Authors:</strong> Charlie B. Tan, Avishek Joey Bose, Chen Lin, Leon Klein, Michael M. Bronstein, Alexander Tong</p>
                    <p>  Scalable sampling of molecular states in thermodynamic equilibrium is a
long-standing challenge in statistical physics. Boltzmann generators tackle
this problem by pairing powerful normalizing flows with importance sampling to
obtain statistically independent samples under the target distribution. In this
paper, we extend the Boltzmann generator framework and introduce Sequential
Boltzmann generators (SBG) with two key improvements. The first is a highly
efficient non-equivariant Transformer-based normalizing flow operating directly
on all-atom Cartesian coordinates. In contrast to equivariant continuous flows
of prior methods, we leverage exactly invertible non-equivariant architectures
which are highly efficient both during sample generation and likelihood
computation. As a result, this unlocks more sophisticated inference strategies
beyond standard importance sampling. More precisely, as a second key
improvement we perform inference-time scaling of flow samples using annealed
Langevin dynamics which transports samples toward the target distribution
leading to lower variance (annealed) importance weights which enable higher
fidelity resampling with sequential Monte Carlo. SBG achieves state-of-the-art
performance w.r.t. all metrics on molecular systems, demonstrating the first
equilibrium sampling in Cartesian coordinates of tri, tetra, and hexapeptides
that were so far intractable for prior Boltzmann generators.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.18462v1" target="_blank">Read PDF</a></p>
                    <p><strong>Submitted on:</strong> 2/25/2025</p>
                </li>
                <li>
                    <h3>DRAMA: Diverse Augmentation from Large Language Models to Smaller Dense
  Retrievers</h3>
                    <p><strong>Authors:</strong> Xueguang Ma, Xi Victoria Lin, Barlas Oguz, Jimmy Lin, Wen-tau Yih, Xilun Chen</p>
                    <p>  Large language models (LLMs) have demonstrated strong effectiveness and
robustness while fine-tuned as dense retrievers. However, their large parameter
size brings significant inference time computational challenges, including high
encoding costs for large-scale corpora and increased query latency, limiting
their practical deployment. While smaller retrievers offer better efficiency,
they often fail to generalize effectively with limited supervised fine-tuning
data. In this work, we introduce DRAMA, a training framework that leverages
LLMs to train smaller generalizable dense retrievers. In particular, we adopt
pruned LLMs as the backbone and train on diverse LLM-augmented data in a
single-stage contrastive learning setup. Experiments show that DRAMA offers
better multilingual and long-context capabilities than traditional
encoder-based retrievers, and achieves strong performance across multiple tasks
and languages. These highlight the potential of connecting the training of
smaller retrievers with the growing advancements in LLMs, bridging the gap
between efficiency and generalization.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.18460v1" target="_blank">Read PDF</a></p>
                    <p><strong>Submitted on:</strong> 2/25/2025</p>
                </li>
                <li>
                    <h3>Supervised Reward Inference</h3>
                    <p><strong>Authors:</strong> Will Schwarzer, Jordan Schneider, Philip S. Thomas, Scott Niekum</p>
                    <p>  Existing approaches to reward inference from behavior typically assume that
humans provide demonstrations according to specific models of behavior.
However, humans often indicate their goals through a wide range of behaviors,
from actions that are suboptimal due to poor planning or execution to behaviors
which are intended to communicate goals rather than achieve them. We propose
that supervised learning offers a unified framework to infer reward functions
from any class of behavior, and show that such an approach is asymptotically
Bayes-optimal under mild assumptions. Experiments on simulated robotic
manipulation tasks show that our method can efficiently infer rewards from a
wide variety of arbitrarily suboptimal demonstrations.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.18447v1" target="_blank">Read PDF</a></p>
                    <p><strong>Submitted on:</strong> 2/25/2025</p>
                </li>
                <li>
                    <h3>olmOCR: Unlocking Trillions of Tokens in PDFs with Vision Language
  Models</h3>
                    <p><strong>Authors:</strong> Jake Poznanski, Jon Borchardt, Jason Dunkelberger, Regan Huff, Daniel Lin, Aman Rangapur, Christopher Wilhelm, Kyle Lo, Luca Soldaini</p>
                    <p>  PDF documents have the potential to provide trillions of novel, high-quality
tokens for training language models. However, these documents come in a
diversity of types with differing formats and visual layouts that pose a
challenge when attempting to extract and faithfully represent the underlying
content for language model use. We present olmOCR, an open-source Python
toolkit for processing PDFs into clean, linearized plain text in natural
reading order while preserving structured content like sections, tables, lists,
equations, and more. Our toolkit runs a fine-tuned 7B vision language model
(VLM) trained on a sample of 260,000 pages from over 100,000 crawled PDFs with
diverse properties, including graphics, handwritten text and poor quality
scans. olmOCR is optimized for large-scale batch processing, able to scale
flexibly to different hardware setups and convert a million PDF pages for only
$190 USD. We release all components of olmOCR including VLM weights, data and
training code, as well as inference code built on serving frameworks including
vLLM and SGLang.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.18443v1" target="_blank">Read PDF</a></p>
                    <p><strong>Submitted on:</strong> 2/25/2025</p>
                </li>
                <li>
                    <h3>ToMCAT: Theory-of-Mind for Cooperative Agents in Teams via Multiagent
  Diffusion Policies</h3>
                    <p><strong>Authors:</strong> Pedro Sequeira, Vidyasagar Sadhu, Melinda Gervasio</p>
                    <p>  In this paper we present ToMCAT (Theory-of-Mind for Cooperative Agents in
Teams), a new framework for generating ToM-conditioned trajectories. It
combines a meta-learning mechanism, that performs ToM reasoning over teammates'
underlying goals and future behavior, with a multiagent denoising-diffusion
model, that generates plans for an agent and its teammates conditioned on both
the agent's goals and its teammates' characteristics, as computed via ToM. We
implemented an online planning system that dynamically samples new trajectories
(replans) from the diffusion model whenever it detects a divergence between a
previously generated plan and the current state of the world. We conducted
several experiments using ToMCAT in a simulated cooking domain. Our results
highlight the importance of the dynamic replanning mechanism in reducing the
usage of resources without sacrificing team performance. We also show that
recent observations about the world and teammates' behavior collected by an
agent over the course of an episode combined with ToM inferences are crucial to
generate team-aware plans for dynamic adaptation to teammates, especially when
no prior information is provided about them.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.18438v1" target="_blank">Read PDF</a></p>
                    <p><strong>Submitted on:</strong> 2/25/2025</p>
                </li>
                <li>
                    <h3>Comparative Analysis of MDL-VAE vs. Standard VAE on 202 Years of
  Gynecological Data</h3>
                    <p><strong>Authors:</strong> Paula Santos</p>
                    <p>  This study presents a comparative evaluation of a Variational Autoencoder
(VAE) enhanced with Minimum Description Length (MDL) regularization against a
Standard Autoencoder for reconstructing high-dimensional gynecological data.
The MDL-VAE exhibits significantly lower reconstruction errors (MSE, MAE, RMSE)
and more structured latent representations, driven by effective KL divergence
regularization. Statistical analyses confirm these performance improvements are
significant. Furthermore, the MDL-VAE shows consistent training and validation
losses and achieves efficient inference times, underscoring its robustness and
practical viability. Our findings suggest that incorporating MDL principles
into VAE architectures can substantially improve data reconstruction and
generalization, making it a promising approach for advanced applications in
healthcare data modeling and analysis.
</p>
                    <p><a href="http://arxiv.org/abs/2502.18412v1" target="_blank">Read PDF</a></p>
                    <p><strong>Submitted on:</strong> 2/25/2025</p>
                </li>
                <li>
                    <h3>Chemical abundance ratios for the bulge of M31</h3>
                    <p><strong>Authors:</strong> F. La Barbera, A. Vazdekis, A. Pasquali</p>
                    <p>  We present abundance ratio estimates of individual elements, namely C, N, Na,
and the so-called alpha elements, Mg, O, Si, Ca, and Ti, for the bulge of M31.
The analysis is based on long-slit, high-quality spectroscopy of the bulge,
taken with the OSIRIS spectrograph at the Gran Telescopio CANARIAS (GTC).
Abundance ratios, [X/Fe]s, are inferred by comparing radially binned spectra of
M31 with different state-of-the-art stellar population models, averaging out
results from various methods, namely full-spectral, full-index, and
line-strength fitting, respectively. For the bulk of the bulge, we find that O,
N, and Na are significantly enhanced compared to Fe, with abundances of about
0.3dex, followed by C, Mg, and Si, with [X/Fe] about 0.2dex, and lastly, Ti and
Ca, mostly tracking Fe ([X/Fe]<0.1dex), within the error bars. Performing the
same analysis on SDSS stacked spectra of early-type galaxies with different
velocity dispersion, we find that the abundance pattern of the M31 bulge is
very similar to that of most massive galaxies, supporting a scenario where most
of the bulge formed in a fast and intense episode of star-formation.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.18409v1" target="_blank">Read PDF</a></p>
                    <p><strong>Submitted on:</strong> 2/25/2025</p>
                </li>
                <li>
                    <h3>The Gradient of Algebraic Model Counting</h3>
                    <p><strong>Authors:</strong> Jaron Maene, Luc De Raedt</p>
                    <p>  Algebraic model counting unifies many inference tasks on logic formulas by
exploiting semirings. Rather than focusing on inference, we consider learning,
especially in statistical-relational and neurosymbolic AI, which combine
logical, probabilistic and neural representations. Concretely, we show that the
very same semiring perspective of algebraic model counting also applies to
learning. This allows us to unify various learning algorithms by generalizing
gradients and backpropagation to different semirings. Furthermore, we show how
cancellation and ordering properties of a semiring can be exploited for more
memory-efficient backpropagation. This allows us to obtain some interesting
variations of state-of-the-art gradient-based optimisation methods for
probabilistic logical models. We also discuss why algebraic model counting on
tractable circuits does not lead to more efficient second-order optimization.
Empirically, our algebraic backpropagation exhibits considerable speed-ups as
compared to existing approaches.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.18406v1" target="_blank">Read PDF</a></p>
                    <p><strong>Submitted on:</strong> 2/25/2025</p>
                </li>
                <li>
                    <h3>Enhancing DNA Foundation Models to Address Masking Inefficiencies</h3>
                    <p><strong>Authors:</strong> Monireh Safari, Pablo Millan Arias, Scott C. Lowe, Lila Kari, Angel X. Chang, Graham W. Taylor</p>
                    <p>  Masked language modelling (MLM) as a pretraining objective has been widely
adopted in genomic sequence modelling. While pretrained models can successfully
serve as encoders for various downstream tasks, the distribution shift between
pretraining and inference detrimentally impacts performance, as the pretraining
task is to map [MASK] tokens to predictions, yet the [MASK] is absent during
downstream applications. This means the encoder does not prioritize its
encodings of non-[MASK] tokens, and expends parameters and compute on work only
relevant to the MLM task, despite this being irrelevant at deployment time. In
this work, we propose a modified encoder-decoder architecture based on the
masked autoencoder framework, designed to address this inefficiency within a
BERT-based transformer. We empirically show that the resulting mismatch is
particularly detrimental in genomic pipelines where models are often used for
feature extraction without fine-tuning. We evaluate our approach on the
BIOSCAN-5M dataset, comprising over 2 million unique DNA barcodes. We achieve
substantial performance gains in both closed-world and open-world
classification tasks when compared against causal models and bidirectional
architectures pretrained with MLM tasks.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.18405v1" target="_blank">Read PDF</a></p>
                    <p><strong>Submitted on:</strong> 2/25/2025</p>
                </li>
                <li>
                    <h3>Kitsune: Enabling Dataflow Execution on GPUs</h3>
                    <p><strong>Authors:</strong> Michael Davies, Neal Crago, Karthikeyan Sankaralingam, Stephen W. Keckler</p>
                    <p>  State of art DL models are growing in size and complexity, with many modern
models also increasing in heterogeneity of behavior. GPUs are still the
dominant platform for DL applications, relying on a bulk-synchronous execution
model which has many drawbacks and is ill-suited for the graph structure of DL
applications. Many industry and academic works attempt to overcome these by
employing vertical fusion but this approach still fails to realize three
untapped opportunities: (1) the fact that many resources on the GPU are idle
while only one operator executes due to temporal multiplexing of the SM; (2)
lower energy from more intelligent on-chip data-movement which lends to higher
performance in a power-provisioned environment. (3) inability to exploit hidden
or reduction dimensions as a source of parallelism to ease pressure on batch
size. This paper explores relatively uncharted territory, answering the
following key question: Can modest adjustments to the current GPU architecture
enable efficient dataflow execution, thereby circumventing the constraints of
vertical fusion without necessitating a clean-slate architecture design. We
develop Kitsune -- a set of primitives that enable dataflow execution on GPUs
and an end-to-end compiler based on PyTorch Dynamo. Across 5 challenge
applications, Kitsune can provide 1.3$\times$-2.3$\times$ and
1.1$\times$-2.4$\times$ performance improvement as well as 41%-98% and 16%-42%
off-chip traffic reduction for inference and training, respectively.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.18403v1" target="_blank">Read PDF</a></p>
                    <p><strong>Submitted on:</strong> 2/25/2025</p>
                </li>
            </ul>
        </section>
    </main>
    <script src="scripts/papers.js"></script>
</body>
</html>