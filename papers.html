<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Papers</title>
    <link rel="stylesheet" href="styles/style.css">
</head>
<body>
    <div class="dog-container">
        <img src="images/dog.jpg" alt="My dog" class="dog-image">
        <p class="dog-caption">The guardian is observing</p>
    </div>
    <header>
        <h1>Papers</h1>
        <nav>
            <ul>
                <li><a href="main.html">Main (regular stuff)</a></li>
                <li><a href="games.html">Games (fun stuff)</a></li>
                <li><a href="papers.html">Papers (sciency stuff)</a></li>
                <li><a href="temperatures.html">Temperatures</a></li>
            </ul>
        </nav>
    </header>
    <main>
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section id="papers-section">
            <h2>Latest Papers in Causal Inference</h2>
            <ul id="papersList">
                <li>
                    <h3>Think Before Recommend: Unleashing the Latent Reasoning Power for
  Sequential Recommendation</h3>
                    <p><strong>Authors:</strong> Jiakai Tang, Sunhao Dai, Teng Shi, Jun Xu, Xu Chen, Wen Chen, Wu Jian, Yuning Jiang</p>
                    <p>  Sequential Recommendation (SeqRec) aims to predict the next item by capturing
sequential patterns from users' historical interactions, playing a crucial role
in many real-world recommender systems. However, existing approaches
predominantly adopt a direct forward computation paradigm, where the final
hidden state of the sequence encoder serves as the user representation. We
argue that this inference paradigm, due to its limited computational depth,
struggles to model the complex evolving nature of user preferences and lacks a
nuanced understanding of long-tail items, leading to suboptimal performance. To
address this issue, we propose \textbf{ReaRec}, the first inference-time
computing framework for recommender systems, which enhances user
representations through implicit multi-step reasoning. Specifically, ReaRec
autoregressively feeds the sequence's last hidden state into the sequential
recommender while incorporating special reasoning position embeddings to
decouple the original item encoding space from the multi-step reasoning space.
Moreover, we introduce two lightweight reasoning-based learning methods,
Ensemble Reasoning Learning (ERL) and Progressive Reasoning Learning (PRL), to
further effectively exploit ReaRec's reasoning potential. Extensive experiments
on five public real-world datasets and different SeqRec architectures
demonstrate the generality and effectiveness of our proposed ReaRec.
Remarkably, post-hoc analyses reveal that ReaRec significantly elevates the
performance ceiling of multiple sequential recommendation backbones by
approximately 30\%-50\%. Thus, we believe this work can open a new and
promising avenue for future research in inference-time computing for sequential
recommendation.
</p>
                    <p><a href="http://arxiv.org/pdf/2503.22675v1" target="_blank">Read PDF</a></p>
                    <p><strong>Submitted on:</strong> 3/28/2025</p>
                </li>
                <li>
                    <h3>A Unified Approach for Estimating Various Treatment Effects in Causal
  Inference</h3>
                    <p><strong>Authors:</strong> Kuan-Hsun Wu, Li-Pang Chen</p>
                    <p>  In this paper, we introduce a unified estimator to analyze various treatment
effects in causal inference, including but not limited to the average treatment
effect (ATE) and the quantile treatment effect (QTE). The proposed estimator is
developed under the statistical functional and cumulative distribution function
structure, which leads to a flexible and robust estimator and covers some
frequent treatment effects. In addition, our approach also takes variable
selection into account, so that informative and network structure in
confounders can be identified and be implemented in our estimation procedure.
The theoretical properties, including variable selection consistency and
asymptotic normality of the statistical functional estimator, are established.
Various treatment effects estimations are also conducted in numerical studies,
and the results reveal that the proposed estimator generally outperforms the
existing methods and is more efficient than its competitors.
</p>
                    <p><a href="http://arxiv.org/pdf/2503.22616v1" target="_blank">Read PDF</a></p>
                    <p><strong>Submitted on:</strong> 3/28/2025</p>
                </li>
                <li>
                    <h3>Generative Latent Neural PDE Solver using Flow Matching</h3>
                    <p><strong>Authors:</strong> Zijie Li, Anthony Zhou, Amir Barati Farimani</p>
                    <p>  Autoregressive next-step prediction models have become the de-facto standard
for building data-driven neural solvers to forecast time-dependent partial
differential equations (PDEs). Denoise training that is closely related to
diffusion probabilistic model has been shown to enhance the temporal stability
of neural solvers, while its stochastic inference mechanism enables ensemble
predictions and uncertainty quantification. In principle, such training
involves sampling a series of discretized diffusion timesteps during both
training and inference, inevitably increasing computational overhead. In
addition, most diffusion models apply isotropic Gaussian noise on structured,
uniform grids, limiting their adaptability to irregular domains. We propose a
latent diffusion model for PDE simulation that embeds the PDE state in a
lower-dimensional latent space, which significantly reduces computational
costs. Our framework uses an autoencoder to map different types of meshes onto
a unified structured latent grid, capturing complex geometries. By analyzing
common diffusion paths, we propose to use a coarsely sampled noise schedule
from flow matching for both training and testing. Numerical experiments show
that the proposed model outperforms several deterministic baselines in both
accuracy and long-term stability, highlighting the potential of diffusion-based
approaches for robust data-driven PDE learning.
</p>
                    <p><a href="http://arxiv.org/pdf/2503.22600v1" target="_blank">Read PDF</a></p>
                    <p><strong>Submitted on:</strong> 3/28/2025</p>
                </li>
                <li>
                    <h3>A Framework for Cryptographic Verifiability of End-to-End AI Pipelines</h3>
                    <p><strong>Authors:</strong> Kar Balan, Robert Learney, Tim Wood</p>
                    <p>  The increasing integration of Artificial Intelligence across multiple
industry sectors necessitates robust mechanisms for ensuring transparency,
trust, and auditability of its development and deployment. This topic is
particularly important in light of recent calls in various jurisdictions to
introduce regulation and legislation on AI safety. In this paper, we propose a
framework for complete verifiable AI pipelines, identifying key components and
analyzing existing cryptographic approaches that contribute to verifiability
across different stages of the AI lifecycle, from data sourcing to training,
inference, and unlearning. This framework could be used to combat
misinformation by providing cryptographic proofs alongside AI-generated assets
to allow downstream verification of their provenance and correctness. Our
findings underscore the importance of ongoing research to develop cryptographic
tools that are not only efficient for isolated AI processes, but that are
efficiently `linkable' across different processes within the AI pipeline, to
support the development of end-to-end verifiable AI technologies.
</p>
                    <p><a href="http://arxiv.org/pdf/2503.22573v1" target="_blank">Read PDF</a></p>
                    <p><strong>Submitted on:</strong> 3/28/2025</p>
                </li>
                <li>
                    <h3>Benchmarking Ultra-Low-Power $Î¼$NPUs</h3>
                    <p><strong>Authors:</strong> Josh Millar, Yushan Huang, Sarab Sethi, Hamed Haddadi, Anil Madhavapeddy</p>
                    <p>  Efficient on-device neural network (NN) inference has various advantages over
cloud-based processing, including predictable latency, enhanced privacy,
greater reliability, and reduced operating costs for vendors. This has sparked
the recent rapid development of microcontroller-scale NN accelerators, often
referred to as neural processing units ($\mu$NPUs), designed specifically for
ultra-low-power applications.
  In this paper we present the first comparative evaluation of a number of
commercially-available $\mu$NPUs, as well as the first independent benchmarks
for several of these platforms. We develop and open-source a model compilation
framework to enable consistent benchmarking of quantized models across diverse
$\mu$NPU hardware. Our benchmark targets end-to-end performance and includes
model inference latency, power consumption, and memory overhead, alongside
other factors. The resulting analysis uncovers both expected performance trends
as well as surprising disparities between hardware specifications and actual
performance, including $\mu$NPUs exhibiting unexpected scaling behaviors with
increasing model complexity. Our framework provides a foundation for further
evaluation of $\mu$NPU platforms alongside valuable insights for both hardware
designers and software developers in this rapidly evolving space.
</p>
                    <p><a href="http://arxiv.org/pdf/2503.22567v1" target="_blank">Read PDF</a></p>
                    <p><strong>Submitted on:</strong> 3/28/2025</p>
                </li>
                <li>
                    <h3>Niyama : Breaking the Silos of LLM Inference Serving</h3>
                    <p><strong>Authors:</strong> Kanishk Goel, Jayashree Mohan, Nipun Kwatra, Ravi Shreyas Anupindi, Ramachandran Ramjee</p>
                    <p>  The widespread adoption of Large Language Models (LLMs) has enabled diverse
applications with very different latency requirements. Existing LLM serving
frameworks rely on siloed infrastructure with coarse-grained workload
segregation -- interactive and batch -- leading to inefficient resource
utilization and limited support for fine-grained Quality-of-Service (QoS)
differentiation. This results in operational inefficiencies, over-provisioning
and poor load management during traffic surges.
  We present Niyama, a novel QoS-driven inference serving system that enables
efficient co-scheduling of diverse workloads on shared infrastructure. Niyama
introduces fine-grained QoS classification allowing applications to specify
precise latency requirements, and dynamically adapts scheduling decisions based
on real-time system state. Leveraging the predictable execution characteristics
of LLM inference, Niyama implements a dynamic chunking mechanism to improve
overall throughput while maintaining strict QoS guarantees. Additionally,
Niyama employs a hybrid prioritization policy that balances fairness and
efficiency, and employs selective request relegation that enables graceful
service degradation during overload conditions. Our evaluation demonstrates
that Niyama increases serving capacity by 32% compared to current siloed
deployments, while maintaining QoS guarantees. Notably, under extreme load, our
system reduces SLO violations by an order of magnitude compared to current
strategies.
</p>
                    <p><a href="http://arxiv.org/pdf/2503.22562v1" target="_blank">Read PDF</a></p>
                    <p><strong>Submitted on:</strong> 3/28/2025</p>
                </li>
                <li>
                    <h3>Approximate stationarity in disjunctive optimization: concepts,
  qualification conditions, and application to MPCCs</h3>
                    <p><strong>Authors:</strong> Isabella KÃ¤ming, Patrick Mehlitz</p>
                    <p>  In this paper, we are concerned with stationarity conditions and
qualification conditions for optimization problems with disjunctive
constraints. This class covers, among others, optimization problems with
complementarity, vanishing, or switching constraints which are notoriously
challenging due to their highly combinatorial structure. The focus of our study
is twofold. First, we investigate approximate stationarity conditions and the
associated strict constraint qualifications which can be used to infer
stationarity of local minimizers. While such concepts are already known in the
context of so-called Mordukhovich-stationarity, we introduce suitable
extensions associated with strong stationarity. Second, a qualification
condition is established which, based on an approximately Mordukhovich- or
strongly stationary point, can be used to infer its Mordukhovich- or strong
stationarity, respectively. In contrast to the aforementioned strict constraint
qualifications, this condition depends on the involved sequences justifying
approximate stationarity and, thus, is not a constraint qualification in the
narrower sense. However, it is much easier to verify as it merely requires to
check the (positive) linear independence of a certain family of gradients. In
fact, it provides a characterization of stationarity and, thus, is weaker than
these strict constraint qualifications. In order to illustrate the obtained
findings, they are applied to optimization problems with complementarity
constraints where they can be naturally extended to the well-known concepts of
weak and Clarke-stationarity.
</p>
                    <p><a href="http://arxiv.org/pdf/2503.22551v1" target="_blank">Read PDF</a></p>
                    <p><strong>Submitted on:</strong> 3/28/2025</p>
                </li>
                <li>
                    <h3>SafeCast: Risk-Responsive Motion Forecasting for Autonomous Vehicles</h3>
                    <p><strong>Authors:</strong> Haicheng Liao, Hanlin Kong, Bin Rao, Bonan Wang, Chengyue Wang, Guyang Yu, Yuming Huang, Ruru Tang, Chengzhong Xu, Zhenning Li</p>
                    <p>  Accurate motion forecasting is essential for the safety and reliability of
autonomous driving (AD) systems. While existing methods have made significant
progress, they often overlook explicit safety constraints and struggle to
capture the complex interactions among traffic agents, environmental factors,
and motion dynamics. To address these challenges, we present SafeCast, a
risk-responsive motion forecasting model that integrates safety-aware
decision-making with uncertainty-aware adaptability. SafeCast is the first to
incorporate the Responsibility-Sensitive Safety (RSS) framework into motion
forecasting, encoding interpretable safety rules--such as safe distances and
collision avoidance--based on traffic norms and physical principles. To further
enhance robustness, we introduce the Graph Uncertainty Feature (GUF), a
graph-based module that injects learnable noise into Graph Attention Networks,
capturing real-world uncertainties and enhancing generalization across diverse
scenarios. We evaluate SafeCast on four real-world benchmark datasets--Next
Generation Simulation (NGSIM), Highway Drone (HighD), ApolloScape, and the
Macao Connected Autonomous Driving (MoCAD)--covering highway, urban, and
mixed-autonomy traffic environments. Our model achieves state-of-the-art (SOTA)
accuracy while maintaining a lightweight architecture and low inference
latency, underscoring its potential for real-time deployment in safety-critical
AD systems.
</p>
                    <p><a href="http://arxiv.org/pdf/2503.22541v1" target="_blank">Read PDF</a></p>
                    <p><strong>Submitted on:</strong> 3/28/2025</p>
                </li>
                <li>
                    <h3>LIM: Large Interpolator Model for Dynamic Reconstruction</h3>
                    <p><strong>Authors:</strong> Remy Sabathier, Niloy J. Mitra, David Novotny</p>
                    <p>  Reconstructing dynamic assets from video data is central to many in computer
vision and graphics tasks. Existing 4D reconstruction approaches are limited by
category-specific models or slow optimization-based methods. Inspired by the
recent Large Reconstruction Model (LRM), we present the Large Interpolation
Model (LIM), a transformer-based feed-forward solution, guided by a novel
causal consistency loss, for interpolating implicit 3D representations across
time. Given implicit 3D representations at times $t_0$ and $t_1$, LIM produces
a deformed shape at any continuous time $t\in[t_0,t_1]$, delivering
high-quality interpolated frames in seconds. Furthermore, LIM allows explicit
mesh tracking across time, producing a consistently uv-textured mesh sequence
ready for integration into existing production pipelines. We also use LIM, in
conjunction with a diffusion-based multiview generator, to produce dynamic 4D
reconstructions from monocular videos. We evaluate LIM on various dynamic
datasets, benchmarking against image-space interpolation methods (e.g., FiLM)
and direct triplane linear interpolation, and demonstrate clear advantages. In
summary, LIM is the first feed-forward model capable of high-speed tracked 4D
asset reconstruction across diverse categories.
</p>
                    <p><a href="http://arxiv.org/pdf/2503.22537v1" target="_blank">Read PDF</a></p>
                    <p><strong>Submitted on:</strong> 3/28/2025</p>
                </li>
                <li>
                    <h3>An integrated method for clustering and association network inference</h3>
                    <p><strong>Authors:</strong> Jeanne Tous, Julien Chiquet</p>
                    <p>  We consider high dimensional Gaussian graphical models inference. These
models provide a rigorous framework to describe a network of statistical
dependencies between entities, such as genes in genomic regulation studies or
species in ecology. Penalized methods, including the standard Graphical-Lasso,
are well-known approaches to infer the parameters of these models. As the
number of variables in the model (of entities in the network) grow, the network
inference and interpretation become more complex. We propose Normal-Block, a
new model that clusters variables and consider a network at the cluster level.
Normal-Block both adds structure to the network and reduces its size. We build
on Graphical-Lasso to add a penalty on the network's edges and limit the
detection of spurious dependencies, we also propose a zero-inflated version of
the model to account for real-world data properties. For the inference
procedure, we propose a direct heuristic method and another more rigorous one
that simultaneously infers the clustering of variables and the association
network between clusters, using a penalized variational
Expectation-Maximization approach. An implementation of the model in R, in a
package called normalblockr, is available on github
(https://github.com/jeannetous/normalblockr). We present the results in terms
of clustering and network inference using both simulated data and various types
of real-world data (proteomics, words occurrences on webpages, and microbiota
distribution).
</p>
                    <p><a href="http://arxiv.org/pdf/2503.22467v1" target="_blank">Read PDF</a></p>
                    <p><strong>Submitted on:</strong> 3/28/2025</p>
                </li>
            </ul>
        </section>
    </main>
    <script src="scripts/papers.js"></script>
</body>
</html>