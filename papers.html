<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Papers</title>
    <link rel="stylesheet" href="styles/style.css">
</head>
<body>
    <div class="dog-container">
        <img src="images/dog.jpg" alt="My dog" class="dog-image">
        <p class="dog-caption">The guardian is observing</p>
    </div>
    <header>
        <h1>Papers</h1>
        <nav>
            <ul>
                <li><a href="main.html">Main (regular stuff)</a></li>
                <li><a href="games.html">Games (fun stuff)</a></li>
                <li><a href="papers.html">Papers (sciency stuff)</a></li>
                <li><a href="temperatures.html">Temperatures</a></li>
            </ul>
        </nav>
    </header>
    <main>
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section id="papers-section">
            <h2>Latest Papers in Causal Inference</h2>
            <ul id="papersList">
                <li>
                    <h3>MetaScale: Test-Time Scaling with Evolving Meta-Thoughts</h3>
                    <p><strong>Authors:</strong> Qin Liu, Wenxuan Zhou, Nan Xu, James Y. Huang, Fei Wang, Sheng Zhang, Hoifung Poon, Muhao Chen</p>
                    <p>  One critical challenge for large language models (LLMs) for making complex
reasoning is their reliance on matching reasoning patterns from training data,
instead of proactively selecting the most appropriate cognitive strategy to
solve a given task. Existing approaches impose fixed cognitive structures that
enhance performance in specific tasks but lack adaptability across diverse
scenarios. To address this limitation, we introduce METASCALE, a test-time
scaling framework based on meta-thoughts -- adaptive thinking strategies
tailored to each task. METASCALE initializes a pool of candidate meta-thoughts,
then iteratively selects and evaluates them using a multi-armed bandit
algorithm with upper confidence bound selection, guided by a reward model. To
further enhance adaptability, a genetic algorithm evolves high-reward
meta-thoughts, refining and extending the strategy pool over time. By
dynamically proposing and optimizing meta-thoughts at inference time, METASCALE
improves both accuracy and generalization across a wide range of tasks.
Experimental results demonstrate that MetaScale consistently outperforms
standard inference approaches, achieving an 11% performance gain in win rate on
Arena-Hard for GPT-4o, surpassing o1-mini by 0.9% under style control. Notably,
METASCALE scales more effectively with increasing sampling budgets and produces
more structured, expert-level responses.
</p>
                    <p><a href="http://arxiv.org/pdf/2503.13447v1" target="_blank">Read PDF</a></p>
                    <p><strong>Submitted on:</strong> 3/17/2025</p>
                </li>
                <li>
                    <h3>MaTVLM: Hybrid Mamba-Transformer for Efficient Vision-Language Modeling</h3>
                    <p><strong>Authors:</strong> Yingyue Li, Bencheng Liao, Wenyu Liu, Xinggang Wang</p>
                    <p>  With the advancement of RNN models with linear complexity, the quadratic
complexity challenge of transformers has the potential to be overcome. Notably,
the emerging Mamba-2 has demonstrated competitive performance, bridging the gap
between RNN models and transformers. However, due to sequential processing and
vanishing gradients, RNN models struggle to capture long-range dependencies,
limiting contextual understanding. This results in slow convergence, high
resource demands, and poor performance on downstream understanding and complex
reasoning tasks. In this work, we present a hybrid model MaTVLM by substituting
a portion of the transformer decoder layers in a pre-trained VLM with Mamba-2
layers. Leveraging the inherent relationship between attention and Mamba-2, we
initialize Mamba-2 with corresponding attention weights to accelerate
convergence. Subsequently, we employ a single-stage distillation process, using
the pre-trained VLM as the teacher model to transfer knowledge to the MaTVLM,
further enhancing convergence speed and performance. Furthermore, we
investigate the impact of differential distillation loss within our training
framework. We evaluate the MaTVLM on multiple benchmarks, demonstrating
competitive performance against the teacher model and existing VLMs while
surpassing both Mamba-based VLMs and models of comparable parameter scales.
Remarkably, the MaTVLM achieves up to 3.6x faster inference than the teacher
model while reducing GPU memory consumption by 27.5%, all without compromising
performance. Code and models are released at http://github.com/hustvl/MaTVLM.
</p>
                    <p><a href="http://arxiv.org/pdf/2503.13440v1" target="_blank">Read PDF</a></p>
                    <p><strong>Submitted on:</strong> 3/17/2025</p>
                </li>
                <li>
                    <h3>Deep Belief Markov Models for POMDP Inference</h3>
                    <p><strong>Authors:</strong> Giacomo Arcieri, Konstantinos G. Papakonstantinou, Daniel Straub, Eleni Chatzi</p>
                    <p>  This work introduces a novel deep learning-based architecture, termed the
Deep Belief Markov Model (DBMM), which provides efficient, model-formulation
agnostic inference in Partially Observable Markov Decision Process (POMDP)
problems. The POMDP framework allows for modeling and solving sequential
decision-making problems under observation uncertainty. In complex,
high-dimensional, partially observable environments, existing methods for
inference based on exact computations (e.g., via Bayes' theorem) or sampling
algorithms do not scale well. Furthermore, ground truth states may not be
available for learning the exact transition dynamics. DBMMs extend deep Markov
models into the partially observable decision-making framework and allow
efficient belief inference entirely based on available observation data via
variational inference methods. By leveraging the potency of neural networks,
DBMMs can infer and simulate non-linear relationships in the system dynamics
and naturally scale to problems with high dimensionality and discrete or
continuous variables. In addition, neural network parameters can be dynamically
updated efficiently based on data availability. DBMMs can thus be used to infer
a belief variable, thus enabling the derivation of POMDP solutions over the
belief space. We evaluate the efficacy of the proposed methodology by
evaluating the capability of model-formulation agnostic inference of DBMMs in
benchmark problems that include discrete and continuous variables.
</p>
                    <p><a href="http://arxiv.org/pdf/2503.13438v1" target="_blank">Read PDF</a></p>
                    <p><strong>Submitted on:</strong> 3/17/2025</p>
                </li>
                <li>
                    <h3>Uncovering Utility Functions from Observed Outcomes</h3>
                    <p><strong>Authors:</strong> Marta Grzeskiewicz</p>
                    <p>  Determining consumer preferences and utility is a foundational challenge in
economics. They are central in determining consumer behaviour through the
utility-maximising consumer decision-making process. However, preferences and
utilities are not observable and may not even be known to the individual making
the choice; only the outcome is observed in the form of demand. Without the
ability to observe the decision-making mechanism, demand estimation becomes a
challenging task and current methods fall short due to lack of scalability or
ability to identify causal effects. Estimating these effects is critical when
considering changes in policy, such as pricing, the impact of taxes and
subsidies, and the effect of a tariff. To address the shortcomings of existing
methods, we combine revealed preference theory and inverse reinforcement
learning to present a novel algorithm, Preference Extraction and Reward
Learning (PEARL) which, to the best of our knowledge, is the only algorithm
that can uncover a representation of the utility function that best
rationalises observed consumer choice data given a specified functional form.
We introduce a flexible utility function, the Input-Concave Neural Network
which captures complex relationships across goods, including cross-price
elasticities. Results show PEARL outperforms the benchmark on both noise-free
and noisy synthetic data.
</p>
                    <p><a href="http://arxiv.org/pdf/2503.13432v1" target="_blank">Read PDF</a></p>
                    <p><strong>Submitted on:</strong> 3/17/2025</p>
                </li>
                <li>
                    <h3>xLSTM 7B: A Recurrent LLM for Fast and Efficient Inference</h3>
                    <p><strong>Authors:</strong> Maximilian Beck, Korbinian Pöppel, Phillip Lippe, Richard Kurle, Patrick M. Blies, Günter Klambauer, Sebastian Böck, Sepp Hochreiter</p>
                    <p>  Recent breakthroughs in solving reasoning, math and coding problems with
Large Language Models (LLMs) have been enabled by investing substantial
computation budgets at inference time. Therefore, inference speed is one of the
most critical properties of LLM architectures, and there is a growing need for
LLMs that are efficient and fast at inference. Recently, LLMs built on the
xLSTM architecture have emerged as a powerful alternative to Transformers,
offering linear compute scaling with sequence length and constant memory usage,
both highly desirable properties for efficient inference. However, such
xLSTM-based LLMs have yet to be scaled to larger models and assessed and
compared with respect to inference speed and efficiency. In this work, we
introduce xLSTM 7B, a 7-billion-parameter LLM that combines xLSTM's
architectural benefits with targeted optimizations for fast and efficient
inference. Our experiments demonstrate that xLSTM 7B achieves performance on
downstream tasks comparable to other similar-sized LLMs, while providing
significantly faster inference speeds and greater efficiency compared to Llama-
and Mamba-based LLMs. These results establish xLSTM 7B as the fastest and most
efficient 7B LLM, offering a solution for tasks that require large amounts of
test-time computation. Our work highlights xLSTM's potential as a foundational
architecture for methods building on heavy use of LLM inference. Our model
weights, model code and training code are open-source.
</p>
                    <p><a href="http://arxiv.org/pdf/2503.13427v1" target="_blank">Read PDF</a></p>
                    <p><strong>Submitted on:</strong> 3/17/2025</p>
                </li>
                <li>
                    <h3>SuperBPE: Space Travel for Language Models</h3>
                    <p><strong>Authors:</strong> Alisa Liu, Jonathan Hayase, Valentin Hofmann, Sewoong Oh, Noah A. Smith, Yejin Choi</p>
                    <p>  The assumption across nearly all language model (LM) tokenization schemes is
that tokens should be subwords, i.e., contained within word boundaries. While
providing a seemingly reasonable inductive bias, is this common practice
limiting the potential of modern LMs? Whitespace is not a reliable delimiter of
meaning, as evidenced by multi-word expressions (e.g., "by the way"),
crosslingual variation in the number of words needed to express a concept
(e.g., "spacesuit helmet" in German is "raumanzughelm"), and languages that do
not use whitespace at all (e.g., Chinese). To explore the potential of
tokenization beyond subwords, we introduce a "superword" tokenizer, SuperBPE,
which incorporates a simple pretokenization curriculum into the byte-pair
encoding (BPE) algorithm to first learn subwords, then superwords that bridge
whitespace. This brings dramatic improvements in encoding efficiency: when
fixing the vocabulary size to 200k, SuperBPE encodes a fixed piece of text with
up to 33% fewer tokens than BPE on average. In experiments, we pretrain 8B
transformer LMs from scratch while fixing the model size, vocabulary size, and
train compute, varying *only* the algorithm for learning the vocabulary. Our
model trained with SuperBPE achieves an average +4.0% absolute improvement over
the BPE baseline across 30 downstream tasks (including +8.2% on MMLU), while
simultaneously requiring 27% less compute at inference time. In analysis, we
find that SuperBPE results in segmentations of text that are more uniform in
per-token difficulty. Qualitatively, this may be because SuperBPE tokens often
capture common multi-word expressions that function semantically as a single
unit. SuperBPE is a straightforward, local modification to tokenization that
improves both encoding efficiency and downstream performance, yielding better
language models overall.
</p>
                    <p><a href="http://arxiv.org/pdf/2503.13423v1" target="_blank">Read PDF</a></p>
                    <p><strong>Submitted on:</strong> 3/17/2025</p>
                </li>
                <li>
                    <h3>Optimal Expert Selection for Distributed Mixture-of-Experts at the
  Wireless Edge</h3>
                    <p><strong>Authors:</strong> Shengling Qin, Hai Wu, Hongyang Du, Kaibin Huang</p>
                    <p>  The emergence of distributed Mixture-of-Experts (DMoE) systems, which deploy
expert models at edge nodes, offers a pathway to achieving connected
intelligence in sixth-generation (6G) mobile networks and edge artificial
intelligence (AI). However, current DMoE systems lack an effective expert
selection algorithm to address the simultaneous task-expert relevance and
channel diversity inherent in these systems. Traditional AI or communication
systems focus on either performance or channel conditions, and direct
application of these methods leads to high communication overhead or low
performance. To address this, we propose the DMoE protocol to schedule the
expert inference and inter-expert transmission. This protocol identifies expert
selection and subcarrier allocation as key optimization problems. We formulate
an expert selection problem by incorporating both AI performance and channel
conditions, and further extend it to a Joint Expert and Subcarrier Allocation
(JESA) problem for comprehensive AI and channel management within the DMoE
framework. For the NP-hard expert selection problem, we introduce the Dynamic
Expert Selection (DES) algorithm, which leverages a linear relaxation as a
bounding criterion to significantly reduce search complexity. For the JESA
problem, we discover a unique structural property that ensures asymptotic
optimality in most scenarios. We propose an iterative algorithm that addresses
subcarrier allocation as a subproblem and integrates it with the DES algorithm.
The proposed framework effectively manages the tradeoff between task relevance
and channel conditions through a tunable importance factor, enabling flexible
adaptation to diverse scenarios. Numerical experiments validate the dual
benefits of the proposed expert selection algorithm: high performance and
significantly reduced cost.
</p>
                    <p><a href="http://arxiv.org/pdf/2503.13421v1" target="_blank">Read PDF</a></p>
                    <p><strong>Submitted on:</strong> 3/17/2025</p>
                </li>
                <li>
                    <h3>Causal Emergence 2.0: Quantifying emergent complexity</h3>
                    <p><strong>Authors:</strong> Erik Hoel</p>
                    <p>  Complex systems can be described at myriad different scales, and their causal
workings often have multiscale structure (e.g., a computer can be described at
the microscale of its hardware circuitry, the mesoscale of its machine code,
and the macroscale of its operating system). While scientists study and model
systems across the full hierarchy of their scales, from microphysics to
macroeconomics, there is debate about what the macroscales of systems can
possibly add beyond mere compression. To resolve this longstanding issue, here
a new theory of emergence is introduced wherein the different scales of a
system are treated like slices of a higher-dimensional object. The theory can
distinguish which of these scales possess unique causal contributions, and
which are not causally relevant. Constructed from an axiomatic notion of
causation, the theory's application is demonstrated in coarse-grains of Markov
chains. It identifies all cases of macroscale causation: instances where
reduction to a microscale is possible, yet lossy about causation. Furthermore,
the theory posits a causal apportioning schema that calculates the causal
contribution of each scale, showing what each uniquely adds. Finally, it
reveals a novel measure of emergent complexity: how widely distributed a
system's causal workings are across its hierarchy of scales.
</p>
                    <p><a href="http://arxiv.org/pdf/2503.13395v1" target="_blank">Read PDF</a></p>
                    <p><strong>Submitted on:</strong> 3/17/2025</p>
                </li>
                <li>
                    <h3>Aligned Probing: Relating Toxic Behavior and Model Internals</h3>
                    <p><strong>Authors:</strong> Andreas Waldis, Vagrant Gautam, Anne Lauscher, Dietrich Klakow, Iryna Gurevych</p>
                    <p>  We introduce aligned probing, a novel interpretability framework that aligns
the behavior of language models (LMs), based on their outputs, and their
internal representations (internals). Using this framework, we examine over 20
OLMo, Llama, and Mistral models, bridging behavioral and internal perspectives
for toxicity for the first time. Our results show that LMs strongly encode
information about the toxicity level of inputs and subsequent outputs,
particularly in lower layers. Focusing on how unique LMs differ offers both
correlative and causal evidence that they generate less toxic output when
strongly encoding information about the input toxicity. We also highlight the
heterogeneity of toxicity, as model behavior and internals vary across unique
attributes such as Threat. Finally, four case studies analyzing detoxification,
multi-prompt evaluations, model quantization, and pre-training dynamics
underline the practical impact of aligned probing with further concrete
insights. Our findings contribute to a more holistic understanding of LMs, both
within and beyond the context of toxicity.
</p>
                    <p><a href="http://arxiv.org/pdf/2503.13390v1" target="_blank">Read PDF</a></p>
                    <p><strong>Submitted on:</strong> 3/17/2025</p>
                </li>
                <li>
                    <h3>TimeZero: Temporal Video Grounding with Reasoning-Guided LVLM</h3>
                    <p><strong>Authors:</strong> Ye Wang, Boshen Xu, Zihao Yue, Zihan Xiao, Ziheng Wang, Liang Zhang, Dingyi Yang, Wenxuan Wang, Qin Jin</p>
                    <p>  We introduce TimeZero, a reasoning-guided LVLM designed for the temporal
video grounding (TVG) task. This task requires precisely localizing relevant
video segments within long videos based on a given language query. TimeZero
tackles this challenge by extending the inference process, enabling the model
to reason about video-language relationships solely through reinforcement
learning. To evaluate the effectiveness of TimeZero, we conduct experiments on
two benchmarks, where TimeZero achieves state-of-the-art performance on
Charades-STA. Code is available at https://github.com/www-Ye/TimeZero.
</p>
                    <p><a href="http://arxiv.org/pdf/2503.13377v1" target="_blank">Read PDF</a></p>
                    <p><strong>Submitted on:</strong> 3/17/2025</p>
                </li>
            </ul>
        </section>
    </main>
    <script src="scripts/papers.js"></script>
</body>
</html>