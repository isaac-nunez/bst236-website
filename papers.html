<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Papers</title>
    <link rel="stylesheet" href="styles/style.css">
</head>
<body>
    <div class="dog-container">
        <img src="images/dog.jpg" alt="My dog" class="dog-image">
        <p class="dog-caption">The guardian is observing</p>
    </div>
    <header>
        <h1>Papers</h1>
        <nav>
            <ul>
                <li><a href="main.html">Main (regular stuff)</a></li>
                <li><a href="games.html">Games (fun stuff)</a></li>
                <li><a href="papers.html">Papers (sciency stuff)</a></li>
            </ul>
        </nav>
    </header>
    <main>
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section id="papers-section">
            <h2>Latest Papers in Causal Inference</h2>
            <ul id="papersList">
                <li>
                    <h3>Is That Your Final Answer? Test-Time Scaling Improves Selective Question
  Answering</h3>
                    <p><strong>Authors:</strong> William Jurayj, Jeffrey Cheng, Benjamin Van Durme</p>
                    <p>  Scaling the test-time compute of large language models has demonstrated
impressive performance on reasoning benchmarks. However, existing evaluations
of test-time scaling make the strong assumption that a reasoning system should
always give an answer to any question provided. This overlooks concerns about
whether a model is confident in its answer, and whether it is appropriate to
always provide a response. To address these concerns, we extract confidence
scores during reasoning for thresholding model responses. We find that
increasing compute budget at inference time not only helps models answer more
questions correctly, but also increases confidence in correct responses. We
then extend the current paradigm of zero-risk responses during evaluation by
considering settings with non-zero levels of response risk, and suggest a
recipe for reporting evaluations under these settings.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.13962v1" target="_blank">Read PDF</a></p>
                    <p><strong>Submitted on:</strong> 2/19/2025</p>
                </li>
                <li>
                    <h3>RAG-Gym: Optimizing Reasoning and Search Agents with Process Supervision</h3>
                    <p><strong>Authors:</strong> Guangzhi Xiong, Qiao Jin, Xiao Wang, Yin Fang, Haolin Liu, Yifan Yang, Fangyuan Chen, Zhixing Song, Dengyu Wang, Minjia Zhang, Zhiyong Lu, Aidong Zhang</p>
                    <p>  Retrieval-augmented generation (RAG) has shown great potential for
knowledge-intensive tasks, but its traditional architectures rely on static
retrieval, limiting their effectiveness for complex questions that require
sequential information-seeking. While agentic reasoning and search offer a more
adaptive approach, most existing methods depend heavily on prompt engineering.
In this work, we introduce RAG-Gym, a unified optimization framework that
enhances information-seeking agents through fine-grained process supervision at
each search step. We also propose ReSearch, a novel agent architecture that
synergizes answer reasoning and search query generation within the RAG-Gym
framework. Experiments on four challenging datasets show that RAG-Gym improves
performance by up to 25.6\% across various agent architectures, with ReSearch
consistently outperforming existing baselines. Further analysis highlights the
effectiveness of advanced LLMs as process reward judges and the transferability
of trained reward models as verifiers for different LLMs. Additionally, we
examine the scaling properties of training and inference in agentic RAG. The
project homepage is available at https://rag-gym.github.io/.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.13957v1" target="_blank">Read PDF</a></p>
                    <p><strong>Submitted on:</strong> 2/19/2025</p>
                </li>
                <li>
                    <h3>Neurosymbolic artificial intelligence via large language models and
  coherence-driven inference</h3>
                    <p><strong>Authors:</strong> Steve Huntsman, Jewell Thomas</p>
                    <p>  We devise an algorithm to generate sets of propositions that objectively
instantiate graphs that support coherence-driven inference. We then benchmark
the ability of large language models (LLMs) to reconstruct coherence graphs
from (a straightforward transformation of) propositions expressed in natural
language, with promising results from a single prompt to models optimized for
reasoning. Combining coherence-driven inference with consistency evaluations by
neural models may advance the state of the art in machine cognition.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.13953v1" target="_blank">Read PDF</a></p>
                    <p><strong>Submitted on:</strong> 2/19/2025</p>
                </li>
                <li>
                    <h3>Asteroseismology with TESS: Emergence of Dipole Mode Suppression From
  Subgiants?</h3>
                    <p><strong>Authors:</strong> Shurui Lin, Tanda Li, Shude Mao, Jim Fuller</p>
                    <p>  Dipole mode suppression is an observed behavior of solar-like oscillations in
evolved stars. This study aims to search for depressed dipole modes in giant
stars using data from the Transiting Exoplanet Survey Satellite (TESS) and
investigate when the suppression starts to emerge. We study a sample of 8,651
TESS-evolved stars and find 179 stars with significant dipole mode depression
by comparing the oscillation amplitudes of radial and dipole modes. Notably, 11
of them are located near the base of the red-giant branch, indicating that mode
suppression appears earlier than the point inferred in previous studies with
the Kepler data. These findings provide new evidence for the dipole mode
suppression in giant stars, particularly in subgiants.
</p>
                    <p><a href="http://arxiv.org/abs/2502.13950v1" target="_blank">Read PDF</a></p>
                    <p><strong>Submitted on:</strong> 2/19/2025</p>
                </li>
                <li>
                    <h3>Why Safeguarded Ships Run Aground? Aligned Large Language Models' Safety
  Mechanisms Tend to Be Anchored in The Template Region</h3>
                    <p><strong>Authors:</strong> Chak Tou Leong, Qingyu Yin, Jian Wang, Wenjie Li</p>
                    <p>  The safety alignment of large language models (LLMs) remains vulnerable, as
their initial behavior can be easily jailbroken by even relatively simple
attacks. Since infilling a fixed template between the input instruction and
initial model output is a common practice for existing LLMs, we hypothesize
that this template is a key factor behind their vulnerabilities: LLMs'
safety-related decision-making overly relies on the aggregated information from
the template region, which largely influences these models' safety behavior. We
refer to this issue as template-anchored safety alignment. In this paper, we
conduct extensive experiments and verify that template-anchored safety
alignment is widespread across various aligned LLMs. Our mechanistic analyses
demonstrate how it leads to models' susceptibility when encountering
inference-time jailbreak attacks. Furthermore, we show that detaching safety
mechanisms from the template region is promising in mitigating vulnerabilities
to jailbreak attacks. We encourage future research to develop more robust
safety alignment techniques that reduce reliance on the template region.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.13946v1" target="_blank">Read PDF</a></p>
                    <p><strong>Submitted on:</strong> 2/19/2025</p>
                </li>
                <li>
                    <h3>PINN ME: A Physics-Informed Neural Network Framework for Accurate
  Milne-Eddington Inversions of Solar Magnetic Fields</h3>
                    <p><strong>Authors:</strong> Robert Jarolim, Momchil E. Molnar, Benoit Tremblay, Rebecca Centeno, Matthias Rempel</p>
                    <p>  Spectropolarimetric inversions of solar observations are fundamental for the
estimation of the magnetic field in the solar atmosphere. However, instrumental
noise, computational requirements, and varying levels of physical realism make
it challenging to derive reliable solar magnetic field estimates. In this
study, we present a novel approach for spectropolarimetric inversions based on
Physics Informed Neural Networks (PINNs) to infer the photospheric magnetic
field under the Milne-Eddington approximation (PINN ME). Our model acts as a
representation of the parameter space, mapping input coordinates (t, x, y) to
the respective spectropolarimetric parameters, which are used to synthesize the
corresponding stokes profiles. By iteratively sampling coordinate points,
synthesizing profiles, and minimizing the deviation from the observed stokes
profiles, our method can find the set of Milne-Eddington parameters that best
fit the observations. In addition, we directly include the
point-spread-function to account for instrumental effects. We use a predefined
parameter space as well as synthetic profiles from a radiative MHD simulation
to evaluate the performance of our method and to estimate the impact of
instrumental noise. Our results demonstrate that PINN ME achieves an intrinsic
spatio-temporal coupling, which can largely mitigate observational noise and
provides a memory-efficient inversion even for extended fields-of-view.
Finally, we apply our method to observations and show that our method provides
a high spatial coherence and can resolve small-scale features both in strong-
and weak-field regions.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.13924v1" target="_blank">Read PDF</a></p>
                    <p><strong>Submitted on:</strong> 2/19/2025</p>
                </li>
                <li>
                    <h3>TESS 2: A Large-Scale Generalist Diffusion Language Model</h3>
                    <p><strong>Authors:</strong> Jaesung Tae, Hamish Ivison, Sachin Kumar, Arman Cohan</p>
                    <p>  We introduce TESS 2, a general instruction-following diffusion language model
that outperforms contemporary instruction-tuned diffusion models, as well as
matches and sometimes exceeds strong autoregressive (AR) models. We train TESS
2 by first adapting a strong AR model via continued pretraining with the usual
cross-entropy as diffusion loss, and then performing further instruction
tuning. We find that adaptation training as well as the choice of the base
model is crucial for training good instruction-following diffusion models. We
further propose reward guidance, a novel and modular inference-time guidance
procedure to align model outputs without needing to train the underlying model.
Finally, we show that TESS 2 further improves with increased inference-time
compute, highlighting the utility of diffusion LMs in having fine-grained
controllability over the amount of compute used at inference time. Code and
models are available at https://github.com/hamishivi/tess-2.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.13917v1" target="_blank">Read PDF</a></p>
                    <p><strong>Submitted on:</strong> 2/19/2025</p>
                </li>
                <li>
                    <h3>How Do LLMs Perform Two-Hop Reasoning in Context?</h3>
                    <p><strong>Authors:</strong> Tianyu Guo, Hanlin Zhu, Ruiqi Zhang, Jiantao Jiao, Song Mei, Michael I. Jordan, Stuart Russell</p>
                    <p>  "Socrates is human. All humans are mortal. Therefore, Socrates is mortal."
This classical example demonstrates two-hop reasoning, where a conclusion
logically follows from two connected premises. While transformer-based Large
Language Models (LLMs) can make two-hop reasoning, they tend to collapse to
random guessing when faced with distracting premises. To understand the
underlying mechanism, we train a three-layer transformer on synthetic two-hop
reasoning tasks. The training dynamics show two stages: a slow learning phase,
where the 3-layer transformer performs random guessing like LLMs, followed by
an abrupt phase transitions, where the 3-layer transformer suddenly reaches
$100%$ accuracy. Through reverse engineering, we explain the inner mechanisms
for how models learn to randomly guess between distractions initially, and how
they learn to ignore distractions eventually. We further propose a
three-parameter model that supports the causal claims for the mechanisms to the
training dynamics of the transformer. Finally, experiments on LLMs suggest that
the discovered mechanisms generalize across scales. Our methodologies provide
new perspectives for scientific understandings of LLMs and our findings provide
new insights into how reasoning emerges during training.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.13913v1" target="_blank">Read PDF</a></p>
                    <p><strong>Submitted on:</strong> 2/19/2025</p>
                </li>
                <li>
                    <h3>Lost in Sequence: Do Large Language Models Understand Sequential
  Recommendation?</h3>
                    <p><strong>Authors:</strong> Sein Kim, Hongseok Kang, Kibum Kim, Jiwan Kim, Donghyun Kim, Minchul Yang, Kwangjin Oh, Julian McAuley, Chanyoung Park</p>
                    <p>  Large Language Models (LLMs) have recently emerged as promising tools for
recommendation thanks to their advanced textual understanding ability and
context-awareness. Despite the current practice of training and evaluating
LLM-based recommendation (LLM4Rec) models under a sequential recommendation
scenario, we found that whether these models understand the sequential
information inherent in users' item interaction sequences has been largely
overlooked. In this paper, we first demonstrate through a series of experiments
that existing LLM4Rec models do not fully capture sequential information both
during training and inference. Then, we propose a simple yet effective
LLM-based sequential recommender, called LLM-SRec, a method that enhances the
integration of sequential information into LLMs by distilling the user
representations extracted from a pre-trained CF-SRec model into LLMs. Our
extensive experiments show that LLM-SRec enhances LLMs' ability to understand
users' item interaction sequences, ultimately leading to improved
recommendation performance. Furthermore, unlike existing LLM4Rec models that
require fine-tuning of LLMs, LLM-SRec achieves state-of-the-art performance by
training only a few lightweight MLPs, highlighting its practicality in
real-world applications. Our code is available at
https://github.com/Sein-Kim/LLM-SRec.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.13909v1" target="_blank">Read PDF</a></p>
                    <p><strong>Submitted on:</strong> 2/19/2025</p>
                </li>
                <li>
                    <h3>Partially Observable Gaussian Process Network and Doubly Stochastic
  Variational Inference</h3>
                    <p><strong>Authors:</strong> Saksham Kiroriwal, Julius Pfrommer, Jürgen Beyerer</p>
                    <p>  To reduce the curse of dimensionality for Gaussian processes (GP), they can
be decomposed into a Gaussian Process Network (GPN) of coupled subprocesses
with lower dimensionality. In some cases, intermediate observations are
available within the GPN. However, intermediate observations are often
indirect, noisy, and incomplete in most real-world systems. This work
introduces the Partially Observable Gaussian Process Network (POGPN) to model
real-world process networks. We model a joint distribution of latent functions
of subprocesses and make inferences using observations from all subprocesses.
POGPN incorporates observation lenses (observation likelihoods) into the
well-established inference method of deep Gaussian processes. We also introduce
two training methods for POPGN to make inferences on the whole network using
node observations. The application to benchmark problems demonstrates how
incorporating partial observations during training and inference can improve
the predictive performance of the overall network, offering a promising outlook
for its practical application.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.13905v1" target="_blank">Read PDF</a></p>
                    <p><strong>Submitted on:</strong> 2/19/2025</p>
                </li>
            </ul>
        </section>
    </main>
    <script src="scripts/papers.js"></script>
</body>
</html>