<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Papers</title>
    <link rel="stylesheet" href="styles/style.css">
</head>
<body>
    <div class="dog-container">
        <img src="images/dog.jpg" alt="My dog" class="dog-image">
        <p class="dog-caption">The guardian is observing</p>
    </div>
    <header>
        <h1>Papers</h1>
        <nav>
            <ul>
                <li><a href="main.html">Main (regular stuff)</a></li>
                <li><a href="games.html">Games (fun stuff)</a></li>
                <li><a href="papers.html">Papers (sciency stuff)</a></li>
            </ul>
        </nav>
    </header>
    <main>
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section id="papers-section">
            <h2>Latest Papers in Causal Inference</h2>
            <ul id="papersList">
                <li>
                    <h3>Testing the limits of fine-tuning to improve reasoning in vision
  language models</h3>
                    <p><strong>Authors:</strong> Luca M. Schulze Buschoff, Konstantinos Voudouris, Elif Akata, Matthias Bethge, Joshua B. Tenenbaum, Eric Schulz</p>
                    <p>  Pre-trained vision language models still fall short of human visual
cognition. In an effort to improve visual cognition and align models with human
behavior, we introduce visual stimuli and human judgments on visual cognition
tasks, allowing us to systematically evaluate performance across cognitive
domains under a consistent environment. We fine-tune models on ground truth
data for intuitive physics and causal reasoning and find that this improves
model performance in the respective fine-tuning domain. Furthermore, it can
improve model alignment with human behavior. However, we find that fine-tuning
does not contribute to robust human-like generalization to data with other
visual characteristics or to tasks in other cognitive domains.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.15678v1" target="_blank">Read PDF</a></p>
                    <p><strong>Submitted on:</strong> 2/21/2025</p>
                </li>
                <li>
                    <h3>AutoToM: Automated Bayesian Inverse Planning and Model Discovery for
  Open-ended Theory of Mind</h3>
                    <p><strong>Authors:</strong> Zhining Zhang, Chuanyang Jin, Mung Yao Jia, Tianmin Shu</p>
                    <p>  Theory of Mind (ToM), the ability to understand people's mental variables
based on their behavior, is key to developing socially intelligent agents.
Current approaches to Theory of Mind reasoning either rely on prompting Large
Language Models (LLMs), which are prone to systematic errors, or use rigid,
handcrafted Bayesian Theory of Mind (BToM) models, which are more robust but
cannot generalize across different domains. In this work, we introduce AutoToM,
an automated Bayesian Theory of Mind method for achieving open-ended machine
Theory of Mind. AutoToM can operate in any domain, infer any mental variable,
and conduct robust Theory of Mind reasoning of any order. Given a Theory of
Mind inference problem, AutoToM first proposes an initial BToM model. It then
conducts automated Bayesian inverse planning based on the proposed model,
leveraging an LLM as the backend. Based on the uncertainty of the inference, it
iteratively refines the model, by introducing additional mental variables
and/or incorporating more timesteps in the context. Empirical evaluations
across multiple Theory of Mind benchmarks demonstrate that AutoToM consistently
achieves state-of-the-art performance, offering a scalable, robust, and
interpretable approach to machine Theory of Mind.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.15676v1" target="_blank">Read PDF</a></p>
                    <p><strong>Submitted on:</strong> 2/21/2025</p>
                </li>
                <li>
                    <h3>Automating Curriculum Learning for Reinforcement Learning using a
  Skill-Based Bayesian Network</h3>
                    <p><strong>Authors:</strong> Vincent Hsiao, Mark Roberts, Laura M. Hiatt, George Konidaris, Dana Nau</p>
                    <p>  A major challenge for reinforcement learning is automatically generating
curricula to reduce training time or improve performance in some target task.
We introduce SEBNs (Skill-Environment Bayesian Networks) which model a
probabilistic relationship between a set of skills, a set of goals that relate
to the reward structure, and a set of environment features to predict policy
performance on (possibly unseen) tasks. We develop an algorithm that uses the
inferred estimates of agent success from SEBN to weigh the possible next tasks
by expected improvement. We evaluate the benefit of the resulting curriculum on
three environments: a discrete gridworld, continuous control, and simulated
robotics. The results show that curricula constructed using SEBN frequently
outperform other baselines.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.15662v1" target="_blank">Read PDF</a></p>
                    <p><strong>Submitted on:</strong> 2/21/2025</p>
                </li>
                <li>
                    <h3>Superintelligent Agents Pose Catastrophic Risks: Can Scientist AI Offer
  a Safer Path?</h3>
                    <p><strong>Authors:</strong> Yoshua Bengio, Michael Cohen, Damiano Fornasiere, Joumana Ghosn, Pietro Greiner, Matt MacDermott, SÃ¶ren Mindermann, Adam Oberman, Jesse Richardson, Oliver Richardson, Marc-Antoine Rondeau, Pierre-Luc St-Charles, David Williams-King</p>
                    <p>  The leading AI companies are increasingly focused on building generalist AI
agents -- systems that can autonomously plan, act, and pursue goals across
almost all tasks that humans can perform. Despite how useful these systems
might be, unchecked AI agency poses significant risks to public safety and
security, ranging from misuse by malicious actors to a potentially irreversible
loss of human control. We discuss how these risks arise from current AI
training methods. Indeed, various scenarios and experiments have demonstrated
the possibility of AI agents engaging in deception or pursuing goals that were
not specified by human operators and that conflict with human interests, such
as self-preservation. Following the precautionary principle, we see a strong
need for safer, yet still useful, alternatives to the current agency-driven
trajectory. Accordingly, we propose as a core building block for further
advances the development of a non-agentic AI system that is trustworthy and
safe by design, which we call Scientist AI. This system is designed to explain
the world from observations, as opposed to taking actions in it to imitate or
please humans. It comprises a world model that generates theories to explain
data and a question-answering inference machine. Both components operate with
an explicit notion of uncertainty to mitigate the risks of overconfident
predictions. In light of these considerations, a Scientist AI could be used to
assist human researchers in accelerating scientific progress, including in AI
safety. In particular, our system can be employed as a guardrail against AI
agents that might be created despite the risks involved. Ultimately, focusing
on non-agentic AI may enable the benefits of AI innovation while avoiding the
risks associated with the current trajectory. We hope these arguments will
motivate researchers, developers, and policymakers to favor this safer path.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.15657v1" target="_blank">Read PDF</a></p>
                    <p><strong>Submitted on:</strong> 2/21/2025</p>
                </li>
                <li>
                    <h3>Logit Disagreement: OoD Detection with Bayesian Neural Networks</h3>
                    <p><strong>Authors:</strong> Kevin Raina</p>
                    <p>  Bayesian neural networks (BNNs), which estimate the full posterior
distribution over model parameters, are well-known for their role in
uncertainty quantification and its promising application in out-of-distribution
detection (OoD). Amongst other uncertainty measures, BNNs provide a
state-of-the art estimation of predictive entropy (total uncertainty) which can
be decomposed as the sum of mutual information and expected entropy. In the
context of OoD detection the estimation of predictive uncertainty in the form
of the predictive entropy score confounds aleatoric and epistemic uncertainty,
the latter being hypothesized to be high for OoD points. Despite these
justifications, the mutual information score has been shown to perform worse
than predictive entropy. Taking inspiration from Bayesian variational
autoencoder (BVAE) literature, this work proposes to measure the disagreement
between a corrected version of the pre-softmax quantities, otherwise known as
logits, as an estimate of epistemic uncertainty for Bayesian NNs under mean
field variational inference. The three proposed epistemic uncertainty scores
demonstrate marked improvements over mutual information on a range of OoD
experiments, with equal performance otherwise. Moreover, the epistemic
uncertainty scores perform on par with the Bayesian benchmark predictive
entropy on a range of MNIST and CIFAR10 experiments.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.15648v1" target="_blank">Read PDF</a></p>
                    <p><strong>Submitted on:</strong> 2/21/2025</p>
                </li>
                <li>
                    <h3>Predicting gene essentiality and drug response from perturbation screens
  in preclinical cancer models with LEAP: Layered Ensemble of Autoencoders and
  Predictors</h3>
                    <p><strong>Authors:</strong> Barbara Bodinier, Gaetan Dissez, Linus Bleistein, Antonin Dauvin</p>
                    <p>  Preclinical perturbation screens, where the effects of genetic, chemical, or
environmental perturbations are systematically tested on disease models, hold
significant promise for machine learning-enhanced drug discovery due to their
scale and causal nature. Predictive models can infer perturbation responses for
previously untested disease models based on molecular profiles. These in silico
labels can expand databases and guide experimental prioritization.
  However, modelling perturbation-specific effects and generating robust
prediction performances across diverse biological contexts remain elusive. We
introduce LEAP (Layered Ensemble of Autoencoders and Predictors), a novel
ensemble framework to improve robustness and generalization. LEAP leverages
multiple DAMAE (Data Augmented Masked Autoencoder) representations and LASSO
regressors. By combining diverse gene expression representation models learned
from different random initializations, LEAP consistently outperforms
state-of-the-art approaches in predicting gene essentiality or drug responses
in unseen cell lines, tissues and disease models. Notably, our results show
that ensembling representation models, rather than prediction models alone,
yields superior predictive performance.
  Beyond its performance gains, LEAP is computationally efficient, requires
minimal hyperparameter tuning and can therefore be readily incorporated into
drug discovery pipelines to prioritize promising targets and support
biomarker-driven stratification. The code and datasets used in this work are
made publicly available.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.15646v1" target="_blank">Read PDF</a></p>
                    <p><strong>Submitted on:</strong> 2/21/2025</p>
                </li>
                <li>
                    <h3>Sparks of cognitive flexibility: self-guided context inference for
  flexible stimulus-response mapping by attentional routing</h3>
                    <p><strong>Authors:</strong> Rowan Sommers, Sushrut Thorat, Daniel Anthes, Tim C. Kietzmann</p>
                    <p>  Flexible cognition demands discovering hidden rules to quickly adapt
stimulus-response mappings. Standard neural networks struggle in tasks
requiring rapid, context-driven remapping. Recently, Hummos (2023) introduced a
fast-and-slow learning algorithm to mitigate this shortfall, but its
scalability to complex, image-computable tasks was unclear. Here, we propose
the Wisconsin Neural Network (WiNN), which expands on fast-and-slow learning
for real-world tasks demanding flexible rule-based behavior. WiNN employs a
pretrained convolutional neural network for vision, coupled with an adjustable
"context state" that guides attention to relevant features. If WiNN produces an
incorrect response, it first iteratively updates its context state to refocus
attention on task-relevant cues, then performs minimal parameter updates to
attention and readout layers. This strategy preserves generalizable
representations in the sensory network, reducing catastrophic forgetting. We
evaluate WiNN on an image-based extension of the Wisconsin Card Sorting Task,
revealing several markers of cognitive flexibility: (i) WiNN autonomously
infers underlying rules, (ii) requires fewer examples to do so than control
models reliant on large-scale parameter updates, (iii) can perform
context-based rule inference solely via context-state adjustments-further
enhanced by slow updates of attention and readout parameters, and (iv)
generalizes to unseen compositional rules through context-state inference
alone. By blending fast context inference with targeted attentional guidance,
WiNN achieves "sparks" of flexibility. This approach offers a path toward
context-sensitive models that retain knowledge while rapidly adapting to
complex, rule-based tasks.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.15634v1" target="_blank">Read PDF</a></p>
                    <p><strong>Submitted on:</strong> 2/21/2025</p>
                </li>
                <li>
                    <h3>Probe Pruning: Accelerating LLMs through Dynamic Pruning via
  Model-Probing</h3>
                    <p><strong>Authors:</strong> Qi Le, Enmao Diao, Ziyan Wang, Xinran Wang, Jie Ding, Li Yang, Ali Anwar</p>
                    <p>  We introduce Probe Pruning (PP), a novel framework for online, dynamic,
structured pruning of Large Language Models (LLMs) applied in a batch-wise
manner. PP leverages the insight that not all samples and tokens contribute
equally to the model's output, and probing a small portion of each batch
effectively identifies crucial weights, enabling tailored dynamic pruning for
different batches. It comprises three main stages: probing, history-informed
pruning, and full inference. In the probing stage, PP selects a small yet
crucial set of hidden states, based on residual importance, to run a few model
layers ahead. During the history-informed pruning stage, PP strategically
integrates the probing states with historical states. Subsequently, it
structurally prunes weights based on the integrated states and the PP
importance score, a metric developed specifically to assess the importance of
each weight channel in maintaining performance. In the final stage, full
inference is conducted on the remaining weights. A major advantage of PP is its
compatibility with existing models, as it operates without requiring additional
neural network modules or fine-tuning. Comprehensive evaluations of PP on
LLaMA-2/3 and OPT models reveal that even minimal probing-using just 1.5% of
FLOPs-can substantially enhance the efficiency of structured pruning of LLMs.
For instance, when evaluated on LLaMA-2-7B with WikiText2, PP achieves a 2.56
times lower ratio of performance degradation per unit of runtime reduction
compared to the state-of-the-art method at a 40% pruning ratio. Our code is
available at https://github.com/Qi-Le1/Probe_Pruning.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.15618v1" target="_blank">Read PDF</a></p>
                    <p><strong>Submitted on:</strong> 2/21/2025</p>
                </li>
                <li>
                    <h3>Pick-and-place Manipulation Across Grippers Without Retraining: A
  Learning-optimization Diffusion Policy Approach</h3>
                    <p><strong>Authors:</strong> Xiangtong Yao, Yirui Zhou, Yuan Meng, Liangyu Dong, Lin Hong, Zitao Zhang, Zhenshan Bing, Kai Huang, Fuchun Sun, Alois Knoll</p>
                    <p>  Current robotic pick-and-place policies typically require consistent gripper
configurations across training and inference. This constraint imposes high
retraining or fine-tuning costs, especially for imitation learning-based
approaches, when adapting to new end-effectors. To mitigate this issue, we
present a diffusion-based policy with a hybrid learning-optimization framework,
enabling zero-shot adaptation to novel grippers without additional data
collection for retraining policy. During training, the policy learns
manipulation primitives from demonstrations collected using a base gripper. At
inference, a diffusion-based optimization strategy dynamically enforces
kinematic and safety constraints, ensuring that generated trajectories align
with the physical properties of unseen grippers. This is achieved through a
constrained denoising procedure that adapts trajectories to gripper-specific
parameters (e.g., tool-center-point offsets, jaw widths) while preserving
collision avoidance and task feasibility. We validate our method on a Franka
Panda robot across six gripper configurations, including 3D-printed fingertips,
flexible silicone gripper, and Robotiq 2F-85 gripper. Our approach achieves a
93.3% average task success rate across grippers (vs. 23.3-26.7% for diffusion
policy baselines), supporting tool-center-point variations of 16-23.5 cm and
jaw widths of 7.5-11.5 cm. The results demonstrate that constrained diffusion
enables robust cross-gripper manipulation while maintaining the sample
efficiency of imitation learning, eliminating the need for gripper-specific
retraining. Video and code are available at https://github.com/yaoxt3/GADP.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.15613v1" target="_blank">Read PDF</a></p>
                    <p><strong>Submitted on:</strong> 2/21/2025</p>
                </li>
                <li>
                    <h3>Causal Modeling of fMRI Time-series for Interpretable Autism Spectrum
  Disorder Classification</h3>
                    <p><strong>Authors:</strong> Peiyu Duan, Nicha C. Dvornek, Jiyao Wang, Lawrence H. Staib, James S. Duncan</p>
                    <p>  Autism spectrum disorder (ASD) is a neurological and developmental disorder
that affects social and communicative behaviors. It emerges in early life and
is generally associated with lifelong disabilities. Thus, accurate and early
diagnosis could facilitate treatment outcomes for those with ASD. Functional
magnetic resonance imaging (fMRI) is a useful tool that measures changes in
brain signaling to facilitate our understanding of ASD. Much effort is being
made to identify ASD biomarkers using various connectome-based machine learning
and deep learning classifiers. However, correlation-based models cannot capture
the non-linear interactions between brain regions. To solve this problem, we
introduce a causality-inspired deep learning model that uses time-series
information from fMRI and captures causality among ROIs useful for ASD
classification. The model is compared with other baseline and state-of-the-art
models with 5-fold cross-validation on the ABIDE dataset. We filtered the
dataset by choosing all the images with mean FD less than 15mm to ensure data
quality. Our proposed model achieved the highest average classification
accuracy of 71.9% and an average AUC of 75.8%. Moreover, the inter-ROI
causality interpretation of the model suggests that the left precuneus, right
precuneus, and cerebellum are placed in the top 10 ROIs in inter-ROI causality
among the ASD population. In contrast, these ROIs are not ranked in the top 10
in the control population. We have validated our findings with the literature
and found that abnormalities in these ROIs are often associated with ASD.
</p>
                    <p><a href="http://arxiv.org/pdf/2502.15595v1" target="_blank">Read PDF</a></p>
                    <p><strong>Submitted on:</strong> 2/21/2025</p>
                </li>
            </ul>
        </section>
    </main>
    <script src="scripts/papers.js"></script>
</body>
</html>